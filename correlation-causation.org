#+TITLE:     Correlation and Causation
#+OPTIONS:   H:2
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}

* Joint and Conditional Probabilities
** Setting
#+ATTR_BEAMER: :overlay <+->
*** Outline
#+ATTR_BEAMER: :overlay <+->
- Random variables $x, y$ taking values in $\mathcal{X}, {Y}$.
- We write $\Pr(x,y)$ to informally mean their joint distribution.

*** Formal definition
#+ATTR_BEAMER: :overlay <+->
- Underlying probability space $P, \Omega, \Sigma$ with
- Outcome space $\Omega$
- Event space $\Sigma$, so that $A \in \Sigma$ are subsets of $\Omega$.
- Probability measure $P : \Sigma \to \Omega$.
- RVs $x : \Omega \to \mathcal{X}$, $y : \Omega \to \mathcal{Y}$
- Joint measure $P_{x,y}(S_x, S_y) \defn P(\{\omega : x(\omega) \in S_x, y(\omega) \in S_y\})$

** Conditional probabilities
#+ATTR_BEAMER: :overlay <+->
**** Definition
The conditional distribution of $x$ given $y$ is:
\[
\Pr(x | y) = \Pr(x, y) / \Pr(y)
\]
Thus, for every value of $y$ we get a different distribution for $x$.

**** Recall definition for events
This has the same form.
\[
P(A | B) = P(A \cap B) / P(B).
\]

** Discrete $x, y$

*** Bernoulli-distributed $x, y \in \{0,1\}$
- $\Pr(x = 1) = \theta$
- $\Pr(y = 1 | x = 0) = v_0$
- $\Pr(y = 1 | x = 1) = v_1$
- $\Pr(x = 1) = ?$

*** One draw from x, y
#+BEGIN_SRC python
  import numpy as  np
  theta = 0.8
  v = np.zeros(2)
  v[0] = 0.1
  v[1]= 0.9
  x = np.random.choice(2, p = [1 - theta, theta])
  y = np.random.choice(2, p = [1 - v[x], v[x]])
  return x,y
#+END_SRC

#+RESULTS:
  
** Python example: multiple draws
#+BEGIN_SRC python
  import numpy as np
  n = 10000
  theta = 0.6
  v = np.zeros(2)
  v[0] = 0.4
  v[1] = 0.8
  x = np.random.choice(2, p = [1 - theta, theta], size = n)
  y = np.array([np.random.choice(2, p = [1 - v[x_t], v[x_t]]) for x_t in x])
  import matplotlib.pyplot as plt
  A = np.zeros([2,2])

  for i in range(2):
	for j in range(2):
	  A[i,j] = sum((x==i) & (y==j))

  plt.imshow(A)
  plt.savefig("correlated-binary.png")
  plt.show()
  return A
#+END_SRC

#+RESULTS:
| 1775 |  208 |
|  804 | 7213 |

** Python example: independent RVs
#+BEGIN_SRC python
  import numpy as np
  n = 10000
  theta = 0.6
  v = 0.8
  x = np.random.choice(2, p = [1 - theta, theta], size = n)
  y = np.random.choice(2, p = [1 - v, v], size = n)
  import matplotlib.pyplot as plt
  A = np.zeros([2,2])

  for i in range(2):
	for j in range(2):
	  A[i,j] = sum((x==i) & (y==j))

  plt.imshow(A)
  plt.savefig("independent-binary.png")
  plt.show()
  return A
#+END_SRC

#+RESULTS:
|  800 | 3221 |
| 1192 | 4787 |


** Empirical joint probability of x, y
	
#+CAPTION: Here $x \sim Bern(0.8)$ and $y \sim Bern(0.9 x)$.
#+NAME:   fig:dependent
[[./correlated-binary.png]]

** Empirical joint probability of x, y
	
#+CAPTION: Here $x \sim Bern(0.8)$ and $y \sim Bern(0.1)$.
#+NAME:   fig:dependent
[[./independent-binary.png]]


* Correlation and independence
** Correlation versus dependence
#+ATTR_BEAMER: :overlay <+->
*** Dependent random variables
#+ATTR_BEAMER: :overlay <+->
- $x, y$ are independent if $\Pr(x,y) = \Pr(x)\Pr(y)$
- equivalently, if $\Pr(x | y) = \Pr(x)$
- $x, y$ are dependent if they are not independent.

*** Correlated random variables
#+ATTR_BEAMER: :overlay <+->
- $x, y$ are uncorrelated if $\E(x,y) = \E(x)\E(y)$
- Equivalently, if $\E(x | y) = \E(x)$
- $x, y$ are correlated if $\E(x,y) \neq \E(x)\E(y)$

*** Theorem
- If $x, y$ are correlated then they are dependent.
- If $x, y$ are independent the they are uncorrelated.



