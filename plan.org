#+TITLE: Digital Skills
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+TAGS: activity advanced definition exercise homework project dataset example theory statistics plot code

* Introduction

This is a hands-on course that integrates introductory programming,
statistics and data science. Through out this course, we will
formulate scientific hypotheses, design experiments, and collect and
analyse data visually and through formal models. You are expected to
supplement this course with homework, self-study and other courses in
descriptive statistics and python programming, but no prior knowledge
is assumed. Formal concepts will be introduced through class
activities and examples.

The course will focus neither on statistical theory nor on
programming. We will only lightly build those skills within the
course. Our main aim will be to build scientific and statistical
intuition through practical work. However, to achieve this you need to
be able to independently pick up theoretical and programming
skills. For that reason, it is necessary for you to do outside reading
either (ideally) by taking statistics and programming courses in
parallel, or through self-study.


** Learning goals:
#+BEGIN_CENTER

*Graphical comprehension*

1. Recognise structural elements in a statistical graph (e.g. axis,
   symbols, labels) and evaluate the effectiveness (for perception and
   judgment) and appropriateness (for the type of data) of structural
   element.
2. Translate relationships reflected in a graph to the data
   represented.
3. Recognise when one graph is more useful than another and
   organise/reorganise data to make an alternative representation.
4. Use context to make sense of what is presented in a graph and avoid
   reading too much into any relationships observed.
5. Express creative thinking via the production of an innovative
   graphical presentation.

*Scientific process*

1. Understanding the randomness, variability and uncertainty inherent
   in a problem.
2. Developing clear statements of the problem/scientific research
   question; understanding the purpose of the answer.
3. Be able to perform a basic experiment design.
4. Identify sources of bias in data collection and analysis.
5. Ensuring acquisition of high-quality data and not just a lot of
   numbers.
6. Understanding the process that produced the data, to provide proper
   context for analysis.
7. Allowing domain knowledge to guide both data collection and
   analysis.
8. Quantify uncertainty---and knowledge---visually.
9. Realise that all visualisations are model summaries.
10. Be able to write simple python programs for data science
    workflows.

#+END_CENTER

** Administration

1. Make sure you are registered on IS-Academia
2. Also register on Moodle: this is where the assignments will be
3. Clone this git repository
   
** Assessment

The assessment is purely through in-class exercises, quizzes and
homework assignments. There will be assignments spread over the
semester, as well as a group project. The project will be performed in
pairs.

For all assignment and the project, the following rubrik is used. Some
of the assignments may not involve all parts.

*Experiment design.* The first stage any project, no matter how small,
is the experiment design and analysis. This includes a plan for how to
collect data, methodologies for analysing the data, and the
development of a pipeline, preferrably in the form of a program, for
collecting data and analysing it. In additional, the experiment design
must be reproducible: This can be ensured by running the data
collection and analysis pipeline on simulated data, and seeing if the
results are as expected.

*Computation.* Here you must instantiate the experiment design and
analysis with concrete computations. For reproducibility, the
computations you perform should be independent of the data you
actually have. Correctness of the computations is the most important
aspect, here. However, you should also take care to document why and
how how you are doing the computations.

*Graphics.* This addresses the creation of visualisations of your
analysis. It is recommended to do this fully automatically, so that
you can simply run your pipeline and get all the results you need.
Be sure to quantify uncertainty.

*Text.* Here you should explain in text what the graphics mean.  Point
out any interesting things you can see in the visualisation and try to
explain it. Do not be overconfident, but quantify uncertainty
properly.

*Synthesis.* Here you should summarise the most important findings
from your analysis. Be careful to not over-interpret your results. A
lot of results can be imaginary and can be attributed to insufficient
data, biased sampling, improper modelling or $p$-value hacking. Again,
be sure to quantify uncertainty.

#+ATTR_LATEX: :align p{3cm}|p{3cm}|p{3cm}|p{3cm} :font \scriptsize
|-----------------------------------------------------------------------------+----------------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------------------------|
| Skill                                                                       | Needs Improvement                                                    | Basic Level                                                             | Advanced Level                                                               |
|-----------------------------------------------------------------------------+----------------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------------------------|
| <25>                                                                        | <25>                                                                 | <25>                                                                    | <25>                                                                         |
| Experiment design: Data collection and analysis pipeline                    | Inappropriate sampling, non-reproducible analysis                    | Data collection biased or analysis not reproducibile.                   | Unbiased sampling and reproducibile experiment desgin and analysis.          |
| Computation: Perform computations                                           | Computations contain errors and extraneous code                      | Computations are correct but contain extraneous/unnecessary code.       | Computations are correct and properly justified and explained.               |
| Graphics: Communicate findings graphically clearly, precisely and concisely | Inappropriate choice of plots; poorly labelled plots; plots missing. | Plots convey information correctly but lack context for interpretation. | Plots convey information correctly with adequate and appropriate information |
| Text: Communicate findings clearly, precisely and concisely                 | Explanation is illogical, incorrect or incoherent.                   | Explanation is partially correct but incomplete or unconvincing         | Explanation is correct, complete and convincing.                             |
| Synthesis: Identify key features of the analysiand interpret results        | Conclusions are missing, incorrect, or not made based on analysis    | Conclsions reasonable, but partially correct or incomplete.             | Relevant conclusions explicitly connected to analysis and context.           |
|-----------------------------------------------------------------------------+----------------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------------------------|


*Pass*: All parts must be addressed, the 'default' grade is 75%. 5% is
added for every 'advanced' skill and removed for every 'needs
improvement skill'. Thus the passing grades are 50-100%.

*Fail*: If not all parts are explicitly addresed, the assignment is failed.
 
** Data sources

This course will consider the following data sources in order of importance.
*** Synthetic data

This data is obtained through simulation, and it is useful in order to
test whether a particular pipeline is working as intended. In
particular, it is a great way to test the performance of a method as
you vary the data generation process so that different assumptions are
satisfied. This allows you to verify robustness.

*** UCI machine learning repository

The [[https://archive.ics.uci.edu/ml/datasets.php][UCI repository]] has a large collection of datasets in an easy to
access format. These have already been used in many academic papers,
and are a good starting point for you to look at real data. All the
data is formatted in an easy-to-use some format, but some
pre-processing may still be necessary.

*** Wikipedia and newspaper articles

Wikipedia has many interesting articles, from which you can extra
tabular data, as well as more contextual information. It is possible
to also discuss newspaper articles. Wikipedia and newspaper articles
can be used in the context of some assignments.

*** Economics data

- [[https://fred.stlouisfed.org/][FRED]]: Federal Reserve Economic Data
- [[https://data.oecd.org/][OECD]]: Organisation for Economic Co-operation and Development

*** PET Statistics Hackathon
https://petlab.officialstatistics.org/
*** NASA Mars Challenge

	https://www.drivendata.org/competitions/97/nasa-mars-gcms/

* Module 1: Visualisation as models and data summary (4 weeks)

 What is visualisation? It is a way to /summarise data/. It is also a way
 to view relationships between variables. Visualisation helps us to
 find patterns and understand the underlying laws behind how the data
 was generated. This is, in fact, the essence of modelling.

 A model is /also/ a way of summarising the essential features of the
 data. A visualisation differs from a model only in one sense: It easy
 to interpret visually. 

 Every data visualisation implicitly assumes a model of the data
 generating process. This is true for even the simplest visualisations,
 like histograms. There is no escape from the fact that any
 visualisation makes a lot of assumptions. We must emphasize what those
 assumptions are. What happens if they are not true?

 Every data visusalisation, then, proceeds in three steps:

 1. Data transformation
 2. Model creation
 3. Model visualisation

*Parameters.* Every model is defined by a number of parameters. This is what is
 displayed when we visualise data. You can think of the model as the
 underlying theory, and the visualisation as a way to explain the
 theory visually.

 
** Histograms: model a distribution

    Histograms are a simple tool for modelling distributions. In their
 simplest application, they are used to simply count the number of items
 in distinct bins of a dataset. While typically employed to represent
 the empirical distribution of one-dimensional variables, they can be
 generalised to multiple dimensions .

*** Bar graph activity
 1. All students who are male raise their hand
 2. All students who are female raise their hand.
 3. We count, and draw a bar graph: the number of male, female and other students.
 4. We count how many are in the BSc of DS of those
 5. We also count how many are taking a programming course
 7. We also count how many are taking a maths/stats course
 6. What does the graph tell us about:
    1. Computer Science students
    2. Students at Neuchatel
    3. Residents of Neuchatel
    4. Other subsets of the population
 7. Can we make a similar graph when measuring a continuous variable? 

**** Definition of a bar graph                                       :theory:

Consider a set of $k$ categories $C = \{1, \ldots, k\}$. Every
individual belongs in one category. This can be defined through a
function $f$ assigning categories to individuals.

In particular, imagine a dataset of individuals $D$. There exists a membership function
\[
f : D \to C
\]
so that $f(x)$ is the category to which individual $x \in D$ belongs.

A bar graph is a visual representation of the following count vector,
\[
n_c(D) = \sum_{i \in C} \ind{f(i) = c},
\]
where $\mathbb{I}$ is an indicator function:
\[
\ind{A} = \begin{cases}
1, & \textrm{$A$ is true}\\
0, & \textrm{$A$ is false},
\end{cases}
\]
so that the height of the $c$-th bar is proportional to $n_c$.

*** Introduction to histograms							  :theory:statistics:
	
 Assume data is in $\mathbb{R}$. Then split the real line into intervals
 $[a_i, b_i)$. For a given dataset $D$, for each interval $i$, count the
 amount of data $n_i(D)$ in the interval. We can also normalise to
 obtain $p_i(D) = n_i(D) / \sum_j n_i(D)$

 More generlaly, a (counting) histogram is defined as a collection of disjoint sets called *bins*
	
 $\{ A_i | i=1, \ldots, k\}$

 with associated counts $n_i$, so that, given some data $D$,

 $n_i(D) = \sum_{x \in D} \mathbb{I}[x \in A_i]$,

 where $n_i$ is the number of datapoints in $A_i$. Typically $A_i \subset R$.

 We can use the histogram as the model of a distribution. For that,
 we use the relative frequency of points in each bin: $p_i(D) =
 n_i(D) / \sum_{j} n_j(D)$.  The selection of bins influences the
 model.

See also: https://en.wikipedia.org/wiki/Histogram


**** Histogram activity											   :activity:
 1. Introduce the concept of a histogram on the board.
 2. Split the students in two groups.
 3. Have each group collect the height of every student.
 4. How can we summarise the data of each group? 
 5. Now the students will individually draw a histogram from the data of their group.
 6. Show two different histograms from two people in the same group. Why are they different? Discuss in pairs and then in class.
 7. Now show a histogram from a person in another group. Why are the histograms in the two groups different? Discuss.
 8. Collect the data of all students in the [[https://docs.google.com/spreadsheets/d/1iMTe4UvVBIS7UZgjYh5Vx7RfgecjFovx5iR4v9TYLJE/edit?usp=sharing][online excel file]].
 9. Now we shall plot a histogram of the students using the sheet. How does that differ?

[If there are not enough students, the exercise can be performed by adding random numbers using dice]

**** Measuring a discrete distribution                             :activity:

1. Toss a coin 10 times and record each one of the results, e.g. {0,1,1,0,0,0,0,1,1,1}.
2. Count the number of times it comes heads or tails.
3. We then summarise the result.

Let us denote the number of times you have heads by $N(x = k)$. It
should be approximately true that $N(x = k) \approx 5$, however, this
may not be true for everybody.

We can visualise this by plotting bars or lines, whose height is
proportional to $N(x=k)$. 

We typically assume that individual coin tosses are generated from [[*The Bernoulli distribution.][The
Bernoulli distribution.]] This means that the probability of heads or
tails is fixed, and does not depend on the result of the previous
tosses.  Why might not that be the case?

If individual tosses are Bernoulli, then the distribution of the
number of heads (or tails) is a [[*Binomial distribution][binomial distribution]].

We will now show how to achieve the same results programmatically.

*** General python help :code:python:
To help yourself understand python, you can always take a look at the
documentation in [[https://docs.python.org/3/][English]] or [[https://docs.python.org/fr/3/][French]]. To start with, check out the
Tutorial. Then use the library reference for advanced usage.

Sometimes it is quicker to just use the *help* command in the python
console or the *?* command in the jupyter notebook.

Python can be used as a simple calculator
#+BEGIN_EXAMPLE
$ python3
Python 3.8.10 (default, Jun 22 2022, 20:18:18) 
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 1 + 1
2
>>> 2 * 3 + 1
7
>>> 2 * (3 + 1)
8
>>> exit()
#+END_EXAMPLE

This interactive console is the most usual way of playing with python
in the beginning, but it is not useful in general.

*** A simple program

Python programs are executed one statement at a time.  Statements are
separated by newlines. Anything appearing after a # symbol is not
executed.
#+BEGIN_SRC python
print("Hello world") # first statement, with a comment
print("Goodbye, world.") # second statement!
# print("This is not printed") - it is a comment, you see
#+END_SRC

Python programs can be generate text, write and read from files,
access the internet, generate and display or save plots to disc, play
and record music, record images from a camera.....

Before we actually run the program, one of you can play the role of
the python *interpreter*. The interpreter goes through each line of
the program, interprets it and executes it. 

When we execute a program in the console, we are assuming the role of
the intpreter that steps through the program.

Run the above statements in the python interpreter
#+BEGIN_EXAMPLE
$ python3
Python 3.8.10 (default, Jun 22 2022, 20:18:18) 
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print ("Hello world")
Hello world
>>> print ("Goodbye, world")
Goodbye, world
>>> exit()
$
#+END_EXAMPLE

The statements print() and exit() are called functions. Function names
must always be used with parenthesis. The contents of the parentheses
are called *arguments*. Changing the arguments to a function has a
different effect.

We can now try and save the above commands in a file called "pythontest.py" and execute it via 
: python3 pythontest.py

*** Consoles, Scripts and Notebooks

*Console* input is used when you want to have a purely interactive
session to test something

*Script* files are used to save your work and re-run it. They also
allow you to build complex programs from multiple files, where each
file has a different functionality.

*Notebooks* are something in between. They are script files with
interactive output, and are very useful for rapid development and
testing. They also save their state and output in between runs, so
they help to document your code.  We will use them a lot in
class. There are two methods to use notebooks:
- Locally through e.g. jupyter-lab or jupyter-notebook
- Online through https://colab.research.google.com/ https://replit.com or https://noto.epfl.ch

Most of the time, you want to be saving the code you write in a script
file and executing it, instead of using the console. However,
sometimes the interactivity of the console is helpful. This is when
notebooks are used. After you are done developing something with the
notebook, you can then extract what you need in a simple python script.

*** Python variables                                                   :code:

The python interpreter has a *state*. This includes the contents of a
memory where variables are stored, and the current location of the
code pointer, that is which line will be executed next.

*Variables* are alphanumeric references to simple or complex objects. Possible variable names:
- X
- NumberOfApples
- salary
- scratch_variable
- y2

A variable can be assigned a value with the = operator.
: x = 2 # this gives the numeric value of '2' to the variable
Variables must be defined before they can be used for the first time
#+BEGIN_EXAMPLE
>>> x
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'x' is not defined
>>> x = 2
>>> x # typing the name of a variable in the console gives you its value
2
#+END_EXAMPLE


 Numerical Python variables are very simple entities. Let us go through
 this is easy program for a warm-up.
 - x=value; assigns a value to a variable named x
 - print(); displays something in the terminal
 #+BEGIN_SRC python :results output
   x = 1 # a variable
   y = 2 # another variable
   print(x+y) # print the value of this variable sum
   x = y # assignment operation: now x has the same value as y
   print(x) #what would this value be?
   y = 3
   print(x) #is x changed?
 #+END_SRC
 #+RESULTS:



*** Python lists                                                       :code:

 A slightly more compex object are python lists. A list can contain
 anything, and is so very flexible. It can contain numbers, strings,
 or arbitrary 'objects'.

Now check out the first part of the [[file:src/histograms/histogram.ipynb][Histogram example]].
For that we need one line of setup so we can plot stuff.
#+BEGIN_SRC python
import matplotlib.pyplot as plt # this is used for plotting
X=[0,1,0,0,0,0,1];  # list of coin tosses
plt.hist(X) # plot a histogram - this automatically splits everything into bins
#+END_SRC

In reality, the histogram function creates a so-called bar plot
#+BEGIN_SRC python
import matplotlib.pyplot as plt # this is used for plotting
X=[0,1,0,0,0,0,1];  # list of coin tosses
plt.bar(["heads","tails"], [sum(X), len(X) - sum(X)]) # do a bar plot!
#+END_SRC

#+RESULTS:

The following source creates a list of four numbers and returns one element.  Things to unpack here:
- *x[i]* returns the *(i+1)-th* element: we start counting *from 0*
- the *return* statement sends a value back to the whatever started
  the python program: in this case this .org file.
#+BEGIN_SRC python :results value
x = [1, 2, 3, 4]
return x[3] # returns the last element of the list
#+END_SRC

#+RESULTS:
: 4




The following program assigns arrays-values to variables. Now x, y are both lists.
 #+BEGIN_SRC python
 x = [1, 2, 3, 4]
 y = [-1, -2]
 x = y # assignment operation: now x is just a different name for y
 y[0] = 1 # modify the 0th element of y
 return x # what would the value of x be?
 #+END_SRC

 #+RESULTS:
 | 1 | -2 |

 Lists are different in one respect: when we assign one list name to
 another, this does not copy any data. Both names refer to the same
 data. Consequently, if we change the data, it changes for both
 variable names.
 The way to avoid that is to use the *copy()* function.
 #+BEGIN_SRC python
 x = [1, 2, 3, 4]
 y = [-1, -2]
 x = y.copy() # copy operation: now x has a copy of y's data
 y[0] = 1 # modify the 0th element of y
 return x # what would the value of x be?
 #+END_SRC

 #+RESULTS:
 | -1 | -2 |

*** Numpy arrays                                              :advanced:code:

 Because lists are very flexible, they are a bit slow. A special type
 of object, an array, is used to handle lists of numbers. This is not
 defined in basic python, but only in one module called /numpy/. Even
 though basic Python has only a few commands, it has many modules that
 extend the language to perform complex tasks without having to code
 everything from scratch.

 #+BEGIN_SRC python
 import numpy as np
 x = np.array([1, 2, 3, 4])
 y = np.array([-1, -2])
 x = y # assignment operation: 
 y[0] = 1
 return x
 #+END_SRC

 #+RESULTS:
 | 1 | -2 |

*** Python control structures
Sometimes we want to repeat some code. For example, we have two matrices of X, Y values and we wish to plot them:
#+BEGIN_SRC python
import matpolotlib.pyplot as plt
import numpy as np
X = np.random.uniform([10,128])
Y = X + np.random.uniform([10,128])
plt.plot(X[0], Y[0])
plt.plot(X[1], Y[1])

#.... etc - to avoid repetition we can use this:
for t in range(10): # this defines the variable t and it cycles it through the values 0, 1, ..., 9.
    # start of repeated block
    plt.plot(X[t], Y[t])
# end of repeated block - blocks are identified by identation

# we can also loop through a specific list of values
for t in [1, 2, -1]: 
    print(t)
# this should output 1, 2, -1
#+END_SRC

*** Python functions                                                   :code:
Sometimes we want to repeat a complex bit of code in different places. So a loop won't do.
The way to do that is to use a function:
#+BEGIN_SRC python
def return_values = function_name(first_argument, second_argument): # there can be zero or more arguments to a function
    return first_argument + second_argument # this function just returns the sum of its arguments
#+END_SRC
*** Pandas and Histograms										  :plot:code:
    For this, we work on the [[file:src/histograms/histogram.ipynb][Histogram example]].
	
    Pandas is a module for simple and efficient data I/O processing
    and visualisation. The following code snippet demonstrates a
    couple of features.
 #+BEGIN_SRC python
   import pandas as pd # we need to load a library first
   # loading data into pandas creates a data frame df
   df['column-name'] # selects a column
   df.hist() # creates a plot with many histograms
 #+END_SRC

**** Coin example											  :activity:plot:

Plotting is also possible through the matplotlib. This is the module
that pandas uses to plot stuff. It just has a simpler interface for
doing so. But if you want to create custom plots, matplotlib is what
you need to use.

 #+BEGIN_SRC python
 X = [1, 0, 1, 0, 1, 1, 0, 1, 0] # a sequence of coin tosses.
 import matplotlib.pyplot as plt # python has no default plot function, we must IMPORT it
 plt.hist(X) # this function plots the histogram
 #+END_SRC

 Each one of you should predict the result of a number of coin tosses.
 Let us do a histogram of the predictions. This is a binomial distribution.

 1. The students record their data in the [[https://docs.google.com/spreadsheets/d/1iMTe4UvVBIS7UZgjYh5Vx7RfgecjFovx5iR4v9TYLJE/edit?usp%3Dsharing][shared spreadsheet]]
 2. Firstly, plot the histogram of the data with default settings.
 3. What is the eff
 Let us look at the student data: see src/histograms/heights.ipynb

**** Heights example											   :activity:
 
 #+BEGIN_SRC python
 import pandas as pd
 X = pd.read_csv("class-data.csv") # read the data into a DataFrame
 X['Height (cm)'].hist() #directly plot the histogram
 #+END_SRC

*** Histograms vs Pie Charts

 While histograms are good visualisations of distributions on the real
 line, distributions over a discrete set of possible values are
 best-represented by a pie-chart. This especially if there is no
 relation between the different values. As an example, if the values
 are distinct categories, there is no particular reason to order them
 on an axis.

- What are the advantages and disadvantages of pie charts and histograms?

 |--------------------------+-----------+-----------|
 |                          | Histogram | Pie Chart |
 |--------------------------+-----------+-----------|
 | To show proportions      |           |           |
 | For more categories      |           |           |
 | To compare relative size |           |           |
 | For real-valued data     |           |           |
 |--------------------------+-----------+-----------|

- Why is a 3D pie chart never a good idea?

 #+BEGIN_SRC: python
 plt.pie(counts) # plot counts
 #+END_SRC

*** Randomness												  :code:activity:
*Random algorithms using coins*.
 #+BEGIN_SRC python
   y = 0 # y is a variable, with the value zero currently
   import numpy as np # this library has many useful functions
   x = np.random.choice(100) # x takes values 'randomly'. It is a 'random variable'.
   return x # let's see what value it takes
 #+END_SRC
 #+RESULTS:
 : 31

*Uncertainty vs randomness: coin-flipping experiment*
	 1. Everybody flips a coin 10 times.
	 2. Record each throw with 0, 1 in this spreadsheet: https://docs.google.com/spreadsheets/d/1E4bs05HnKXf1GZe4g3v6RLnHsj-YcaWg3Qe_RQyfhHU/edit?usp=sharing
	 3. Then record how you threw the coin and what coin it was.
	 4. Discuss if the coin is really random.
	 5. What is the distribution of coin throws for the first throw?
	 6. What is the distribution of recorded coin biases? Why do some coins appear more biased than others?
	 7. Does it make sense to aggregate all the results? What does that assume?



In the context of experiment design and data analysis, it is very
common to have conditions like those in this example.  Even though we
wish there was such a thing as the 'repeated experiment', in practice
it never is repeated. There is always some varying factor.

*Pseudo-random numbers*

 Let us now repeat the experiment with data generated via a computer.
 #+BEGIN_SRC python
 # here is a default way to generate 'random' numbers
 import random
 X = random.choices([0, 1], k=10) # uniformly choose 10 times between 0 and 1.
 plt.hist(X) # everytime we run these commands, we get a different proportion
 #+END_SRC

 #+RESULTS:

 This python code is completely deterministic. A complicated
 calculation is used to generate the next 'random' number from the
 previous one. Consider this example:
 #+BEGIN_SRC python
 import random
 seed(5) #this sets the 'state' of the random number generating machine
 print(random.uniform(0,1)) # the random number is a function of the state
 print(random.uniform(0,1)) # the state changes after we generate a new number
 print(random.uniform(0,1))
 seed(5) # when we reset the state, we get the same sequence of numbers
 print(random.uniform(0,1)) #
 print(random.uniform(0,1))
 print(random.uniform(0,1))
 #+END_SRC python

 For cryptographically strong random numbers you need to use the secrets module:
 #+BEGIN_SRC python
 import secrets
 secrets.choice(range(100))
 #+END_SRC

*Physical sources of randomness*

 Let's go back to throwing coins now. Coins are completely
 deterministic.  Whenever we have a specific coin to throw in the air,
 there are two things we do not know. The first is which side the coin
 will land on. Why is that? The second is uncertainty about the coin
 bias: is the probability of landing heads exactly 50%? How can we
 quantify this? What does it depend on? Discuss in class.

 What physical source of randomness can we use instead of coins?

*** Uncertainty													   :activity:

Probability is not only used to model random events. In fact, almost
nothing can be said to be really random, unless we go into quantum
physics. Even a die thrown in the air follows precise mechanical
laws. Given enough information, it is possible to accurately predict
the outcome of a throw.

For that reason, probability is best thought of as a way to model any
residual uncertainty we have about an event. Then the probability of
an event is simply a subjective measure of the likelihood. 

While probability offers a nice mathematical formulation of
uncertainty, when this uncertainty is subjective, the question arises:
how can we elicit precise probabilities about uncertain events from
individuals?  Here is an example.

**** The number of immigrants                                      :activity:
 Consider the following question: how many immigrants live in
 Switzerland?  

 1. In-class discussion: what do we mean by that?

 2. Now everybody can make a guess and record it on this form: https://moodle.unine.ch/mod/evoting/view.php?id=295622

 What does this distribution mean? Can we use it as an estimate of uncertainty?

 3. Now let us create some confidence intervals. The procedure is as
 follows. Let us take a first guess at an inteval, (say 5-10%) and ask:
 (a) Are you willing to take an even bet that the true number is between [5-10%]?


  
*** Probability background                                           :theory:probability:

The theory of probability is used to mathematically define processes
with uncertain outcomes. The set of all possible outcomes depends on
the process.  For example, if we throw a die, this is the set of all
possible ways, locations etc that the die can fall and land.  However,
we may only be interested in two events: whether the die lands showing
a '6' or not.  Formally, an event $A$ is a subset of all the possible
outcomes $\Omega$. In our example, $A$ can be the set of all ways in
which the die can land so that its top shows "6". A probability
measure $P$ simply assigns a number between 0 and 1 to every subset
$A$ we might be interested in. This can be thought of as the area of
$A$. Different probability measures $P$ assign different areas to
different sets.

For some more technical details, see [[*Probability space][Probability space]].

See also:
- https://en.wikipedia.org/wiki/Probability

**** Probability space :theory:probability:

In probability theory, we typically define the set of all possible
events that we care about as the algebra $\Sigma$, so that any
possible event $A \in \Sigma$ and so that $A \subset \Omega$.  The
algebra has the property that it is closed under union and complement,
that is:

1. If $A, B \in \Sigma$ then $A \cup B \in \Sigma$
2. If $A \in \Sigma$ then $\neg A \in \Sigma$.

Here, $\neg A \defn \Omega \setminus A$, i.e. the subset of $\Omega$
not containing $A$.

Together with a probability measure $P$, the tuple $(\Omega, \Sigma,
P)$ defines a probability space. Simply put, $P(A)$ is the probability
that event $A$ happens.

**** Probability measures								 :theory:probability:
A probability measure $P$ is a function from sets to the interval
$[0,1]$. Measuring the probability of a set is technically the same as
measuring the area of a region, or the number of items in a given
region. Formally, for a probability measure is defined on:

- A *universe* $\Omega$ of outcomes
- The *algebra* $\Sigma$ of subsets of $\Omega$ (which we can think of
  as all the 'events' of interest) so that:
(a) If $A \in \Sigma$, then $A \subset \Omega$
(b) If $A, B \in \Sigma$ then $A \cup B \in \Sigma$.
(b) If $A \in \Sigma$ then $\Omega \setminus A \in \Sigma$.

*The axioms of probability* A probability measure $P: \Sigma \to
 [0,1]$ on $\Omega$ satisfies the following axioms:
1. $P(\Omega) = 1$.
2. If $A \cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.

From these, it also follows that $P(\emptyset) = 0$.

See also: https://en.wikipedia.org/wiki/Probability_measure

*** Example Distributions

We focus on distributions where there is a finite number of possible
outcomes, and hence a finite number of possible events that we might
care about. All such distributions are characterised by one or more
/parameters/. The simplest such distribution is a distribution on only
two outcomes, the family of Bernoulli distributions.

**** The Bernoulli distribution.                     :definition:probability:
Let us start with a simple example, the Bernoulli distribution with
parameter $\theta \in [0,1]$. This is the distribution over two outcomes
$\{0,1\}$, so that if $x$ is a Bernoulli random variable, then:
\[
\Pr(x = 1) = \theta, \qquad \Pr(x = 0) = 1 - \theta.
\]
It is typical to think the distribution of heads and tails of a coin
as being Bernoulli, with parameter $\theta = 1/2$.

*Probability space* Formally, if the underlying probability space is
$(\Omega, \Sigma, P)$, with random outcomes $\omega \in \Omega$ and random variable $x : \Omega \to \{0,1\}$ then
 \[ \Pr(x = 1) = P(\{\omega : x(\omega) = 1\}).  \]

See also: https://en.wikipedia.org/wiki/Bernoulli_distribution

**** Binomial distribution                           :definition:probability:
If we repeat a Bernouli trial, we can also count the number of times
the coin comes heads. The distribution of the counts is
called the Binomial distribution. If $y$ is a binomial random variable for $n$ throws with parameter $\theta$,
then we can write it as the sum of $n$ Bernoulli random variables $x_1, \ldots, x_n$, i.e.:
\[
y = \sum_{t=1}^n x_t.
\]
The probability of $k$ heads after $n$ throws is given by the formula:
\[
\Pr(y = k) = \binom{n}{k} \theta^{k} (1 - \theta)^{n - k}
\],
where $\binom{n}{k}$ is the bimomial coefficient.

See also: https://en.wikipedia.org/wiki/Binomial_distribution

**** The Categorical/Multinomial distribution

A multinomial distribution is an extension of the Bernoulli and
binomial distributions to $m \geq 2$ outcomes. 

*Categorial distributions* Let us start with
one trial, e.g. a single throw of a die. We can model this dice throw
as the distribution where the probability
that the die lands with its $k$-th face on top is $\theta_k$,
\[ \Pr(x = k) = \theta_k.\]
Thus, this distribution is parametrised by the vector $\theta = (\theta_1, \ldots, \theta_m)$.
A random variable $x : \Omega \to \{1,\ldots, m\}$ obeying this distribution is called
multinomial. 

If the underlying probability space is $(\Omega, \Sigma,
P)$, then
\[ \Pr(x = k) = P(\{\omega : x(\omega) = k\}) \]

See also: https://en.wikipedia.org/wiki/Multinomial_distribution

**** Uniform distributions

A special case of binomial and multinomial distributions is the
uniform distribution. This is defined as follows.

Let $|A|$ be the size of a set $A$. Then a distribution $P$ is uniform if it obeys
\[
P(A) = \frac{|A|}{|\Omega|}.
\]

This definition applies to continuous distributions as well. A standard example is the uniform distribution on the interval $[0,1)$. Then
the probability that we obtain an outcome in the set $[0,p)$ is always equal to $p$, i.e.
\[
P([0,p)) = \Pr(\omega \in [0,p)) = p.
\]



**** Random variables									 :theory:probability:

A real-valued random variable $f : \Omega \to \mathbb{R}$ is simply a
function from the outcomes to the real numbers. Even though it is a
fixed function, its values are random, because the actual value
$\omega \in \Omega$ that will be used to calculate its value
$f(\omega)$ is random.

Random variables can be easily generalised to other domains than the real numbers.

See also: https://en.wikipedia.org/wiki/Random_variable
En francais: https://fr.wikipedia.org/wiki/Variable_al%C3%A9atoire
***** Example random variable                                       :example:

Take a 10-sided die. The outcomes of the die represent the space
$\Omega$. We can now create a Bernoulli variable from the die. For
example, set $x(\omega) = 1$ when $\omega \in \{1, 5\}$ and $x(\omega)
= 0$ when $\omega \in \{6, 10\}$.

What is the distribution of $x$? Have you seen it before?


***** Generating a Bernoulli random variable.                 :activity:code:

In python, there are procedures for generating data from many types of
random variables. However, all such methods for generating random
numbers are not truly random. They rely on something called a
pseudorandom number generator. The values output by this generator are
then /transformed/ so as to become similar to the random variable we
want.

1. Let us start by throwing a 10-sided die. How do you generate a
   Bernoulli random variable with $\theta = 0.6$ from the outcomes of
   the dice throw?

2. Now let us consider the following python code, which generates
   values from a Bernoulli random variable.
#+BEGIN_SRC python
import numpy as np
return np.random.choice(2,p=[0.6, 0.4])
#+END_SRC

#+RESULTS:
: 0

   Let us replicate it through a distribution of uniform randomv variables in $[0,1]$.
#+BEGIN_SRC python
import numpy as np
x = np.random.uniform() # returns a uniformly generated number in [0,1). 
if x < 0.6:
  return 1
else:
  return 0
#+END_SRC

#+RESULTS:
: 1

*** Group comparison                                               :activity:

Let us say we wish to compare the distribution among multiple
groups. Perhaps we wish to compare the number of students who achieve
a certain test like the table below.

|---------+---------+---------|
|         | Success | Success |
|---------+---------+---------|
| School  |    Male |  Female |
|---------+---------+---------|
| A       |     62% |     82% |
| B       |     63% |     68% |
| C       |     37% |     34% |
| D       |     33% |     35% |
| E       |     28% |     24% |
| F       |      6% |      7% |
|---------+---------+---------|
| Average |     45% |     38% |
|---------+---------+---------|

1. Let us plot the success rate for females and males over different schools.
2. Does this show a bias? What information is missing?
3. Let us combine these two plots into one plot now for this we need
   to use the following code:
#+BEGIN_SRC python
w=0.5
X = np.linspace(1,6*w,6)
M = ...
F = ...
plt.bar(X, M, align=edge, width=w/2)
plt.bar(X + w, F, align=edge, width=w/2)
#+END_SRC


** Time-Series: model the evolution of a system

 A time series $x_1, \ldots, x_t$ is simply a sequence of variables. We
 typically assume that this is random. How can we capture this
 dependency between variables? Does the value of $x_t$ depend only on
 the value of $x_{t-1}$? On all the previous values? Only on the time
 index $t$?

 Frequently, sequential observations of a variable $x_t$ are in fact
 noisy measurements of the true variable of interest, $y_t$, which we
 never observe. As an example, consider covid infections. There is a
 true, underlying, number of infections, but we only ever measure the
 number of positive cases detected in a day. 

 Generally, there are three tasks associate with time series
 modelling, always given data up to this point, i.e. $x_1, \ldots,
 x_t$.

 1. Smoothing: What has happened in the past? Here we estimate
    $y_{t-k}$ for $k > 0$.
 2. Filtering: What is the current situation?  To solve this problem
    we must estimate $y_t$.
 3. Prediction: What will happen in the future? This involves
    predicting $y_{t+k}$ for some $k > 0$.

These problems are all related and can be formalised in a statistical
manner, and there are multiple algorithms that can be used to solve
each problem.  When $x_t = y_t$, then smoothing and filtering are
trivial, but prediction is still an important problem.  We focus here
on a simple linear transformation such as the moving average as a basic solution method.

*Smoothing* For smoothing, a moving average filter is typically
sufficient whenever $\mathbb{E}(x_t) = y_t$, i.e. $x$ is just a zero-mean
noisy measurement of $y$. Then we can construct the estimator
\[
\hat{y}_t = \frac{1}{2n+1} \sum_{k = t-n}^{t+n} x_k.
\]

*Filtering* When we wish to filter, at best we can take the moving
average from the past $n$ observations. If $n$ is very large, then
there is a a corresponding delay between our filtering and the final
prediction.
\[
\hat{y}_t = \frac{1}{n+1} \sum_{k = t-n}^{t} x_k.
\]
The only way to remove the lag is to perform a more complex
transformation of the original data. To see this, consider the problem
of prediction.


*Prediction* Prediction means estimating something in the future. This
task is never trivial, even with perfect observations, i.e. when $x_t
= y_t$. In this setting moving averages do not make sense. A simple
idea is to /assume a linear trend/, e.g. that $y_{t+1} - y_t = y_t -
y_{t-1}$. By re-arranging terms, we have that $y_{t+1} = 2y_t - y_{t-1}$.
This gives us the estimator:
\[
\hat{y}_{t+1} = 2x_t - x_{t-1}
\]



*** Plotting lines

 Here is a simple example of line plotting. 
 #+BEGIN_SRC python :results file :var f="example.png"
 import numpy as np
 X = [1, 2, 3, 4, 5, 4, 3, 2, 1] # define a small number of points
 import matplotlib.pyplot as plt # import the plotting library
 plt.plot(x) # perform a standard, simple plot
 plt.savefig(f)
 return f
 #+END_SRC

 What are such plots useful for?

*** Race times													   :activity:
 https://en.wikipedia.org/wiki/1500_metres_world_record_progression

 Wikipedia has a table that shows the progression of 1500m world records.
 1. Let us first [[file:src/time-series/WorldRecords.py][show the records up to 1950]] .  
 2. Try and predict the progrssion of world records on the board.
 3. Let us now look at the actual graph. Is it what you expected?
 4. How do you expect the progression to continue after 2020?
 5. How do you explain this progression? Can you find data to validate or refute your explanation?


**** Scraping tables example :example:data-collection:
 #+BEGIN_SRC python
   import pandas
   tables=pandas.read_html("URL") # read a table
   # convert date-string:
   dt = datetime.datetime.strptime(string, '%Y-%m-%d').year
   # string manipulation
   string.replace("+", "0") # replaces a + with a 0
   string.split(":") # splits a string into multiple strings
   # data formats
   float("12.2"); # converts a number into a float
 #+END_SRC



*** Example: The inclination of Mars								:example:

1. Plot [[file:data/astronomy/mars.xls][Mars data]]
2. Show orbits
3. 3-body system, chaos and randomness
   
*** Example: Covid													:example:

1. Plot covid data.
2. Smooth the data: moving average plots
3. Try and estimate past, current and future infections with simple tools.
4. Discuss: Are those simple tools sufficient? Is our visualisation consistent? Do we need something further?


*** Example: Stock market prices
 See: Trading Economics



** Scatterplots: model a relationship
   
Let us start with an example where we just have three variables.
We can plot the relationship between any two of them.

#+BEGIN_SRC python
X=[1, 2, 3, 4, 10, 6]
Y=[5, 2, 5, 3, 1, 2]
Z=[0, 1, 0, 1, 0, 1]
import matplotlib.pyplot as plt
plt.scatter(X,Y)
#+END_SRC
#+RESULTS:

Variables are frequently in some array instead.
#+BEGIN_SRC python
import numpy as np
n_data = 10
n_features = 3
data = np.random.uniform(size=[n_data, n_features]) # create some random data
plt.scatter(data[:,0], data[:,1]) #plot the first against the second column
# We can always take a 'slice' of the data:
data[:,[1,2]] # get columns 1 and 2 and all the rows
data[1:10, [0,2]] # get columns 0 and 2 and all rows 1-10
## : means everyhing
## a:b means everything from a to b
## [a,b,c] means a, b and c.
#+END_SRC
In dataframes, it we can deal with multiple variables by name

#+BEGIN_SRC python
import pandas as pd
df = pd.DataFrame(data, columns =  ["Alcohol", "Caffeine", "Sugar"])
plt.scatter(df["Alcohol"], df["Caffeine"]) #plot the first against the second column
# getting slices is also possible in pandas dataframes, just slightly different:
df.loc[:,'Alcohol'] # get column Alcohol
df.loc[:,['Alcohol', 'Caffeine']] # get column Alcohol and Caffeine
#+END_SRC


*** Relationships as functions

A lot of relationships between two variables $x \in X$, $y \in Y$, can
be described through some deterministic function $f : X \to Y$, i.e.
\[ y = f(x).\]

If the relationship is one-to-one, then there exists an
inverse function $f^{-1} : Y \to X$, so that
\[
x = f^{-1}(y),
\]
with
\[
x = f^{-1}[f(x)].
\]

Sometimes, however, the relationship betwen the two variables is not
deterministic, that is the value of $x$ does not uniquely determine
the value of $y$, or the converse may occur... or both.

**** Physical relations                                             :example:

Many equations in physics relate two quantities.  For example, there
is the equation relating current $I$, voltage $V$ and resistance $R$:
\[
V = IR.
\]
This relation can be inverted to obtain
\[
R = V/I, \qquad I = V / R.
\]
Let us say that the resistance $R$ is fixed. By altering the voltage
(e.g. by adding more batteries to a circuit) we can see that the
current increases.


*** Relationships as joint distributions

The simplest way to model a stochastic relationship between two
variables is to model the joint distribution $P(X,Y)$. Consider the
example of heights versus weights: We can expect that the taller a
person is, the heavier they will be. However, their weight will depend
on the mass of their muscle and adipose tissue. These in turn depend
on their age, sex, genetics and lifetime calorie expenditure and
intake.

**** Weight and height distribution :example:

Here we can plot the number of people having a certain height and
weight combination. This can be done with a colour-map. This is not
much different than a normal histogram - and is called hist2d in pyplot:
#+BEGIN_SRC
X = 100 / (1 + np.exp(-np.random.normal(size=100))) + 125
Y = X *  (1 + 0.1*abs(np.random.normal(size=100))) - 100
#+END_SRC

*** Relationships as conditional distribution.

Here we model the distribution of one variable given a fixed value for
the other, e.g. $P(X|Y)$. The simplest thing is to only try to model the expected value 
$E[X|Y]$. How can we do this?

Method 1: polynomial fitting!
#+BEGIN_SRC python
  # returns the best fitting line to the data
  a, b = np.polyfit(data_x, data_y, 1)
  # Why is this the 'best' line? Because it minimises the total squared
  # error between the predicted value and the actual ones.
  # We can plot the line by this simple linear equation
  ax = np.linspace(0,1)
  plt.plot(ax, a * ax + b)
#+END_SRC
	
*** Example: Unemployment, GDP

Get some data financial data from FRED.  This is time-series data. Can
we actually make sense out of it in terms of correlations?  Explore.


First, the unemployment rate: https://fred.stlouisfed.org/series/UNRATE
Then, the GDP: https://fred.stlouisfed.org/series/GDPC1
This has two different data frames.
#+BEGIN_SRC python
  # read the files
  import pandas as pd
  ur=pd.read_csv("UNRATE.csv")
  gdp=pd.read_csv("GDPC1.csv")
  # the date ranges are different, so we must try to merge them (inner join!)
  merged = ur.merge(gdp)
#+END_SRC
	
* Module 2: Experiment design  (3 weeks)
** Data collection and cleaning :activity:
** Random sampling
 1. Uniformly random sampling. How can we perform it?
 2. Biased sampling, correcting for effects.
 3. Importance sampling.
*** Survey of political opinions                                   :activity:
You each support one of the following political groups:
Red.
Green.
Blue.

In this exercise, we will try to measure the support for different political parties.

**** Full coverage sampling                                        :activity:
Throw a die for your political affiliation
|-----+-------|
| Die | Party |
|-----+-------|
|   0 | Red   |
|   1 |       |
|-----+-------|
|   2 | Green |
|   3 |       |
|   4 |       |
|   5 |       |
|-----+-------|
|   6 | Blue  |
|   7 |       |
|   8 |       |
|   9 |       |
|-----+-------|

We now count the number of people having different affiliations. This are your true voting affiliations.
If you were to vote, then you would vote for these specific parties.

Given the number of people in the course, what is the expected number of votes for each party? 


**** Connecting probabilities of outcomes to probabilities :probability:theory:example:
From a probability perspective, we can think of the die as having random outcomes in $\Omega = [9]$. 
The random variable is the party vote $v : \Omega \to \{R, G, B\}$ with
\[
v(\omega) = 
\begin{cases}
R, & \omega \in \{0,1\}\\
G, & \omega \in \{2,3,4,5\}\\
B, & \omega \in \{6,7,8,9\}
\end{cases}
\]

Let us assume that the die has a uniform distribution $P$ so that
$P(\omega) = 1/10$ for all $\omega \in \Omega$ and $P(S) = |S|/10$ for
all subsets $S \subset \Omega$.  What is then the probability that
somebody supports the Red party?

The probability that $v = R$, which we write informally as $\Pr(v = R)$,
 is simply the probability of all $\omega$ that lead to $R$, that is:
\[
\Pr(v = R) = P(\{\omega : v(\omega) = R\}) = P(\{0,1\}) = 2.
\]

**** Expectations                                       :probability:theory:example:

If we know the probability that a randomly chosen voter will vote for
the $i$-party, then what is the probability of different numbers of votes?
What is the expected number of votes for each party?

First, let us consider another random variable:

- $n_i$: the number of votes cast for each party $i$.

This total number of votes depends on the party affiliation of each voter. Let
- $\omega_t$ be the random die of person $t$.
- $v_t = v(\omega_t)$ is then the party affiliation of person $t$.

We collect the random die into one big vector
\[
\omega = (\omega_1, \ldots, \omega_t, \ldots, \omega_T), \qquad \omega \in \Omega^T, \omega_t \in \Omega
\]
Then the total number of votes for party $i$ is simply
\[
n_i(\omega) = \sum_{t=1}^T \mathbb{I} \{f_t = i\} = \sum_{t=1}^T \mathbb{I} \{v(\omega_t) = i\}
\]

Clearly, if there is only one voter, the expected number of votes for
each party $i$ is simply: $\Pr(v = i)$. If we had $T$ voters then this
should be $n_i = T \Pr(v = i)$. We can verify this by writing out the
expectation.

 \[ \E_P[n_i] = \sum_\omega P^T(\omega) n_i(\omega) \]

Here $P^T$ is the combined distribution of all persons dice.
The main assumption we must make is that each person's die is independent of everybody else's. Then
\[
P^T(\omega) = \prod_{t=1}^T P(\omega_t).
\]
Consequently, the expectation becomes
\begin{align*}
\E_P[n_i]
&=
\sum_{\omega \in \Omega^T} P^T(\omega) n_i(\omega)\\
&=
\sum_{\omega \in \Omega^T} P^T(\omega) \sum_{j=1}^T \mathbb{I} \{v_j = i\}\\
&=
\sum_{\omega \in \Omega^T} \sum_{j=1}^T P^T(\omega)  \mathbb{I} \{v_j = i\}\\
&=
\sum_{j=1}^T \sum_{\omega \in \Omega^T} [P^T(\omega)  \mathbb{I} \{v_j = i\}]\\
&=
\sum_{j=1}^T P^T(\{\omega : v(\omega_j = i)\})\\
&=
\sum_{j=1}^T P(\{\omega_j : v(\omega_j = i)\})\\
&=
\sum_{j=1}^T P_v(i).
\end{align*}



**** The probability measure induced by a random variable f :probability:theory:advanced:example:
We already define a probability measure $P$ on the outcomes $\Omega$. Since $f$ is a function on $\Omega$, we can also define an appropraite probability measure $P_f$ for the outputs of $f$. In particular, for any subset $S \subset \{R, G, B\}$, we define
\[
P_f(B) = P(\{\omega : f(\omega) \in B\}).
\]
We can this identify the informal probability $\Pr(f = R)$ with the measure $P_f(R)$.
Here, even though $R$ is an element, and not a set, we abuse notation. Normally we would write $P_f(\{R\})$ to denote the set consisting only of R$.


**** Random sampling :activity:

Each one of you throws a second die and records the outcome. We now have
|-----+-------+---------------|
| Die | Party | Response      |
|-----+-------+---------------|
|   0 | Red   | Not Reachable |
| 1-9 |       | Red           |
|-----+-------+---------------|
|   0 | Green | Not Reachable |
| 1-2 |       | Refuse        |
| 3-9 |       | Green         |
|-----+-------+---------------|
| 0-4 | Blue  | Not Reachable |
|   5 |       | Refuse        |
| 6-9 |       | Blue          |
|-----+-------+---------------|

- Make a histogram / bar-char pie plot on the number of votes


** The data science pipeline
 The experimental pipipeline has a number of different components. 
 1. Formulating the problem.
 2. Deciding what type of data is needed.
 3. Choosing the model and visualisation needed.
 4. Designing the experimental protocol.
 5. Generating data confirming to our assumptions.
 6. Testing the protocol on synthetic data. Is it working as expected?
* Module 3: Inference (2 weeks)
** Expectation                                           :theory:probability:
 Recall that a random variable $f$ is a function $f : \Omega \to \mathbb{R}$. 
 The expectation of a random variable with underlying distribution $P(\omega)$ is simply
 \[
 \mathbb{E}_P[f] \defn \sum_{\omega \in \Omega} f(\omega) P(\omega).
 \]
 There is nothing random about the variable itself, it is only the random input that makes its value random.

 #+BEGIN_SRC python
   def random_variable(omega):
       return omega * omega
 #+END_SRC

*** Centime exercise

 A jar with coins is passed around the class. 
 1. The students are asked to guess how many coins it contains.
 2. The students agree on a 50% confidence interval.
 3. The students fit a [[https://en.wikipedia.org/wiki/Normal_distribution][normal distribution]] on this interval $[\mu - \frac{2}{3}\sigma, \mu + \frac{2}{3}\sigma]$.
 4. Is this normal distribution a good choice? Are you 90\% sure the number of coins is less than $x$?
 5. Is a normal distribution generally appropriate?
 6. Puzzle: Guess how many coins there are. If correct, then the class will share the money. If not, they will get nothing. What is the correct guess?
 (If students have trouble with this, try with small numbers of coins and finite number of possibilities - demonstrate by playing the guessing game repeatedly)

** Bayesian analysis                                     :theory:probability:
 Recall the definition of Conditional probability:

 $P(A | B) = P(A \cap B) / P(B)$,

 i.e. the probability of A given B is the probability of A and B happening divided by the probability of B.

 From this it follows that

 $P(B | A) = P(A \cap B) / P(A)$.

 Combining the two equations, we obtain:

 $P(A | B) = P(B | A) P (A) / P(B)$.

 So we can reverse the order of conditioning, i.e. relate to the probability of A given B to that of B given A.

*** The covid test problem                                         :activity:
 10% of the class has covid, i.e. P(covid) = 0.1. Each one of you performs a covid test. If
 you have covid, the test is correct 80% of the time, i.e. P(positive |
 covid) = 0.8. Conversely, if you do not have covid, there is still a
 10% chance of a positive test, with P(positive | not-covid) = 0.1

 How likely is it that you have covid if your test is positive or negative, i.e.
 P(covid | positive), vs. P(covid | negative)?

 First of allzzzzzzzzzzzzzzzz, each one of you should independently generate a uniform random
 number between 1 and 10. For that, you can each throw a die, and record the outcome.

 Then you throw a second die, and record that as well.

 I will now pass over the tables and tell each one of you if they have a positive test.

 Now, everybody with a positive test raises their hand. I expect it to
 be slightly more than 10% (but it depends).

*** The cards problem
 1. Print out a number of cards, with either [A|A], [A|B] or [B|B] on their sides.
 2. Get a card (say with face A), and ask what is the probability the other side is the same.
 3. Have the students perform the experiment with:
    1. Draw a random card.
    2. Count the number of people with A.
    3. Of those, count the number of people with A on the other side.
    4. It should be clear that 1/3 of people have [A|A] and of those 

*** The k-Meteorologists problem

 Bayesian reasoning is most useful in the following setting:

 - We have models of the world, $\{P_\theta | \theta \in \Theta\}$.
 - We have a prior distribution $P(\theta)$ over the models.
 - We obtain data $D$ for whiche very model assigns a probabiltiy $P_\theta(D)$.
 - We calculate the posterior distribution
 $P(\theta | D) = P_\theta(D) P(\theta) / P(D)$.
 - This tells us how likely each model is given the data.

 In this example, we have $k$ meteorological stations, each one of
 which gives us the probability that it will rain. 

 The table below gives the probability of rain according to each
 station.


 #+CAPTION: Rain probabilities and events
 | Station       | Day 1 | Day 2 | Day 3 |
 |---------------+-------+-------+-------|
 | MeteoSuisse   |   70% |       |       |
 | Chris's Model |   50% |       |       |
 |---------------+-------+-------+-------|
 | Actual rain   |       |       |       |
 |---------------+-------+-------+-------|

 The table below is our belief at the beginning of each day, about
 which station is overall best in predicting rain. What should our
 initial belief be?

 #+CAPTION: Belief at start of day
 | Belief        | Day 1 | Day 2 | Day 3 | Day 4 |
 |---------------+-------+-------+-------+-------|
 | MeteoSuisse   |   90% |       |       |       |
 | Chris's Model |   10% |       |       |       |
 |---------------+-------+-------+-------+-------|

 Write a program that updates the beliefs sequentially given
 observations and station predictions.

** Hypothesis testing

*** Homework assignment: Define a data collection and analysis problem
* Module 4: Advanced visualisation (2 weeks)
** Geographical data
 https://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html

*** Colour maps
1. Colour as a continuous variable.
2. Colour as a discrete variable.
3. Colour perception and interpretation.
*** Contour maps
1. Geographical contour
2. Density plots
   	 
** Text data
* Module 5: Data analysis in practice (2 weeks)
** Survey data: The garden of many paths

** Visualising fMRI data
** Visualising GWAS data
*** Homework assignment: Visualisation of a project

* Module 6: Project work and presentations (2 weeks)

* Assignments

The course contains assignments and a project. The instructions for
each assignment are given below. The assignments are largely done in
class, but completed at home.

** Table To Picture
TLDR: Find a table in wikipedia on a topic of interest, and convert the table into a graph.

The purpose of this assignment is for you to create a graphic that
demonstrates something interesting about a data table found on the
web.  Please provide as precise and concise answers as possible. This
assignment is *graded*. You are /encouraged/ to discuss
the assignment with other students in a group. However each student
must prepare their own /individual/ report.  Please submit your
answers on /moodle/

*** Instructions
In this exercise, you must create a plot from an existing dataset and write a short report. Use the following steps as a guideline.
  1. Find a data table on the internet. 
  2. Write a short description of the data on the table.
  3. Create one or two plots of your choice, summarising the data on the table.
  4. Explain what the graph shows about the data.
  5. Try and draw some conclusions or generalisations from the graph. Does it make logical sense?

*** Example
  This is an example of this exercise for a dataset we already saw in class. 

  1. Use the world records data for 100m/400m/1500m or some other distance.
  2. Explain what the records show.
  3. Show how the world record changes over time for men and women,
    with different colours. Be sure to plot records with the x axis
    showing time.
  4. Draw regression lines over the world record graph: the records reduce over time.
  5. It is not logically possible to expect the times to reduce linearly with time! Is there a fundamental limit? How can the data be best explained? Is human performance constant over time, and records are falling due to random chance? Is human performance slowly increasing over time?

** Plot deconstruction

 TLDR: Take an existing plot from the web, re-create it, and try to improve it.

*** Instructions
 Find an inteesting plot from a web page on e.g. wikipedia. Try to
 identify some problem with the plot. To help you, ask yourself the
 following questions:

 - Is the plot type appropriate?
 - Is the data correct?
 - Does the plot convey an appropriate message?
 - Is thee more data somewhere that you could combine with the original to obtain a better picture?

 After you have identified problems with the plot, data sources, or
 missing data, create a new plot, along with an explanation of how you
 addressed the original plot's deficiencies.

*** Example


** Newspaper article analysis

In this assignment you will read a newspaper article with some
statistics and visualisations, and try to interpret what it says.  You
must study the article criticially. Are the conclusions supported by
the data? Does the methodology make sens? Find primary sources that
confirm or challenge the article to obtain a more rounded picture.

Here is a list of possible articles you can use. Feel free to suggest
your own article and add it to the list.

https://docs.google.com/spreadsheets/d/1QKj_L9f0UIH80qgs2kcjc8AU1eKZzsTOcYFSU06HJBY/edit#gid=0

** Simulation study

For a simple visualisation problem, vary parameter values and simulate
thousands of times under each set of conditions. Summarise your
findings graphically.

** Copy the master

You are given a visualisation constructed from a given dataset. You
must create a similar visualisation from another (or the same)x dataset.

** Open project
*** Project proposal
DEADLINE: <2022-10-21 Fr>
Propose a problem to solve, including:
- Hypotheses to test
- How to collect data
- How to analyse the collected data
*** Project Highlight
DEADLINE: <2022-11-04 Fr>
After you have started your project, each one of the project members
presents a preliminary plot and explains it (5 minutes). 

*** Project presentation
DEADLINE: <2022-12-02 Fr>

*** Project report
DEADLINE: <2022-12-16 Fr>

The completed project should include a report written by both students
in the team. This should should address the points in the [[*Assessment][Assessment]]
description.

* Notation
For convenience, I include necessary mathematical notation

** Sets
- $\mathbb{R}$: Real numbers
- $\mathbb{R}^d$: d-dimensional Euclidean space
- $\emptyset$: The empty set
- $A \subset B$: A is a subset of B.
- $A \cap B$: The intersection of A and B
- $A \cup B$: The union of A and B
- $A \setminus B$: Removing B from A
- $\Omega$: The "universe"
- $A^c = \Omega \setminus A$: The complement of a set.
- $\{x | f(x) = 0\}$: The set of x so that $f(x) = 0$.
** Analysis
- $\mathbb{I}\{x \in A\}$: indicator function (takes the value $1$ if $x \in A$, $0$ oterwise)
- $\sum_{x \in X} f(x) = f(x_1) + \cdots + f(x_n)$, with $X = \{x_1, \ldots, x_n\}$
- $d/dx f(x)$: derivative of $f$
- $\partial/\partial x f(x,y)$: partial derivative of $f$
- $\nabla_x = (\partial/\partial x_1, \ldots, \partial/\partial x_n)$, vector of partial derivatives.
** Probability
- $\Pr$: Probability (generally)
- $\mathbb{E}$: Probability
- $P$: A probability measure
- $p$: A probability density
- $P(A | B) = P(A \cup B) / P(B)$. Conditional probability, $A, B \subset \Omega$.
- $\param$: Parameter
- $\Param$: Parameter set
- $\{P_\param | \param \in \Param\}$: A family of parametrised models
- $\Pr(x | y)$ conditional probability for random variables x, y (generally)

* Graphics types
1. Histogram and 3D extensions
2. Density curve
3. Scatterplot
4. Smooth scatterplot
5. Violin plot
6. Line Plot
7. Confidence Intervals
8. Geographical/topological maps
9. Network graphs
10. Word cloud

See also: [[https://datavizcatalogue.com/][Catalogue]] of data visualisation


* Schedule and links to other courses

The schedule of this and the other courses is in flux, but I do not
expect it to change very much.  In any case, the course will operate
independently of the other courses. You should expect to cover the
same topic more than once. However, this course will not focus on
either statistical theory or programming.

|--------+---------------------------+--------------+--------------------------+-----------------------|
| Week   | Statistics                | Programming  | In-Course                | Homework              |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 1      | Course intro              | Python intro | Histograms               |                       |
| 23 Sep |                           |              | Randomness               |                       |
|        |                           |              |                          | Math score            |
|        |                           |              |                          |                       |
|        |                           |              |                          |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 2      | R Intro                   | Data types   |                          | Form groups           |
| 30 Sep | Data manipulation         |              | Uncertainty              |                       |
|        | Histograms                |              | Discrete Variables       |                       |
|        | Scatterplots              |              | Continuous Variables     |                       |
|        | Boxplots                  |              |                          |                       |
|        | Variable types            |              |                          |                       |
|        | Mosaic plots              |              |                          |                       |
|        | Functions                 |              |                          |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 3      | Quantifying Variability   | Control      | Time-Series              |                       |
| 7 Oct  | Distribution              |              | Linear functions         | Form groups           |
|        | Density function          |              | Stock market prices      |                       |
|        | Histograms                |              | Crime statistics         |                       |
|        | Skewness                  |              | S&P index                |                       |
|        | Quantiles                 |              | World Records            |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 4      | Qualitative vars in R     | Structures   |                          |                       |
| 14 Oct | Discrete vars in R        |              |                          | Proj. Proposal        |
|        |                           |              | Scatterplots             |                       |
|        |                           |              | Unemployment             |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 5      | Continous RV              | Functions    |                          |                       |
| 21 Oct |                           |              | Data Cleaning            | Table2Picture         |
|        |                           |              |                          |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 6      | Continuous RV             | Complements  | Experiment design        |                       |
| 28 Oct |                           |              | Random Sampling          | Deconstruction        |
|        |                           |              | Undercounting            |                       |
|        |                           |              | Represnetative samples   |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 7      | Continuous RV             | Classes      | Expectations             |                       |
| 4 Nov  |                           |              |                          | NewsPaper             |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 8      | Dependencies.             | Objects      | Bayesian Inference       |                       |
| 11 Nov | Joint distribution.       |              |                          | Project Highlight     |
|        | Conditional distribution. |              |                          |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 9      | Moments                   | Errors       | Pipelines                | Project               |
| 18 Nov |                           |              | Simulation studies       |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 10     | Covariance                | Iterators    |                          |                       |
| 25 Nov | Correlation               |              | Hypothesis testing       | Project               |
|        | Scatterplots              |              | The garden of many paths |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 11     | Prices, returns           | FP           | Examples, project work   |                       |
| 2 Dec  |                           |              |                          | Project               |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 12     | Conditional expectations  |              |                          | Project presentations |
| 9 Dec  |                           |              | Examples, project work   |                       |
|--------+---------------------------+--------------+--------------------------+-----------------------|
| 13     |                           |              |                          |                       |
| 16 Dec |                           |              | Project presentations    | Project work          |
|--------+---------------------------+--------------+--------------------------+-----------------------|
|        |                           |              |                          |                       |




* Glossary

- Expectation: Esprance
- Histogram: Histogramme?
- Pie plot: ?
- Bar plot: ?
- Scatterplot: ?
- Randomness: Hasard
- Uncertainty: Incertitude
- Probability: Probabilit
- Stochastic: stochastique
- Random Variable: Variable alatoire
- Sample: chantillon
- Sampling: chantillonnage
- Set: Ensemble
- Subset: Sous-ensemble
- Superset: Sur-ensemble
- Survey: Sondage
- $\sigma$-algebra: tribu, $\sigma$-algbre

* References

Python help: Use python's !help! function whenever you can. 
#+BEGIN_SRC python
help(print)
#+END_SRC

** Some workshops
- https://vishub.net/bach
- https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22331
- https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=22351

** Books
- [CA] The Truthful Art. Cairo, Alberto :book:
- [GJ] Data Science Par La Pratique (Data Science from Scratch). Grus, Joel 
 (Ch3: Visualisation, Ch6: Probability, Ch7: Inference, Ch9: Data collection, Ch10: Exploration, Ch14: Regression, Ch15: Regression+Bootstrap)




