* Learning goals
1. Understanding the randomness, variability and uncertainty inherent in a problem.
2. Developing clear statements of the problem/scientific research question; understanding the purpose of the answer
3. Ensuring acquisition of high-quality data and not just a lot of numbers
4. Understanding the process that produced the data, to provide proper context for analysis
5. Allowing domain knowledge to guide both data collection and analysis
#+BEGIN_SRC python
X = [1, 0, 1, 0, 1, 1, 0, 1, 0] # a sequence of coin tosses.
import matplotlib.pyplot as plt # python has no default plot function, we must IMPORT it
plt.hist(X) # this function plots the histogram
#+END_SRC

*** Randomess (45')
 1. Random algorithms using coins.
 2. Uncertainty versus randomness.
 3. Coin-flipping experiment
	1. Everybody flips a coin 10 times.
	2. Record how many heads or tails you have.
	3. Then record how you threw the coin.
	4. Discuss if the coin is really random.
*** Uncertainty (45')
	1. Predicting events. Who will win the race?
	2. How many immigrants live in Switzerland? The students should
       first discuss what we mean by that? Now let us create some confidence
       intervals:
	   1. The simplest method is to create a histogram with everybody's choices.
   
** Time-Series: model a causal effect
*** Race times
https://en.wikipedia.org/wiki/1500_metres_world_record_progression

To scrape tables
#+BEGIN_SRC python
  import pandas
  tables=pandas.read_html("URL") # read a table
  # convert date-string:
  dt = datetime.datetime.strptime(string, '%Y-%m-%d').year
  # string manipulation
  string.replace("+", "0") # replaces a + with a 0
  string.split(":") # splits a string into multiple strings
  # data formats
  float("12.2"); # converts a number into a float

#+END_SRC

*** Example: Stock market prices




*** Example: Crime statistics

*** Example: S&P index

** Scatterplots: model a relationship
   1. For the original data: add weight, eye colour, gender, exercise level.
   2. Summarise
*** Example: Stock market, Unemployment, GDP
	
** Homework Assignment: Take an existing plot from the web, re-create it, and try to improve it.
* Experiment design   
** Random sampling
1. Pure random sampling.
2. Undercounting.
3. Give mode.
** A/B testing
 1. Comparing algorithms in the wild. Which is the best algorithm?
** The data science pipeline
 The experimental pipipeline has a number of different components. 
 1. Formulating the problem.
 2. Deciding what type of data is needed.
 3. Choosing the model and visualisation needed.
 4. Designing the experimental protocol.
 5. Generating data confirming to our assumptions.
 6. Testing the protocol on synthetic data. Is it working as expected?
** Homework Assignment: Analyse Newspaper articles
* Inference
Recall the definition of Conditional probability:

$P(A | B) = P(A \cap B) / P(B)$,

i.e. the probability of A given B is the probability of A and B happening divided by the probability of B.

From this it follows that

$P(B | A) = P(A \cap B) / P(A)$.

Combining the two equations, we obtain:

$P(A | B) = P(B | A) P (A) / P(B)$.

So we can reverse the order of conditioning, i.e. relate to the probability of A given B to that of B given A.


** The cards problem
1. Print out a number of cards, with either [A|A], [A|B] or [B|B] on their sides.
2. Get a card (say with face A), and ask what is the probability the other side is the same.
3. Have the students perform the experiment with:
   1. Draw a random card.
   2. Count the number of people with A.
   3. Of those, count the number of people with A on the other side.
   4. It should be clear that 1/3 of people have [A|A] and of those 

** The k-Meteorologists problem

Bayesian reasoning is most useful in the following setting:

- We have models of the world, $\{P_\theta | \theta \in \Theta\}$.
- We have a prior distribution $P(\theta)$ over the models.
- We obtain data $D$ for whiche very model assigns a probabiltiy $P_\theta(D)$.
- We calculate the posterior distribution
$P(\theta | D) = P_\theta(D) P(\theta) / P(D)$.
- This tells us how likely each model is given the data.

In this example, we have $k$ meteorological stations, each one of
which gives us the probability that it will rain. 

The table below gives the probability of rain according to each
station.

#+NAME: Rain probabilities and events
| Station       | Day 1 | Day 2 | Day 3 |
|---------------+-------+-------+-------|
| MeteoSuisse   |   70% |       |       |
| Chris's Model |   50% |       |       |
|---------------+-------+-------+-------|
| Actual rain   |       |       |       |
|---------------+-------+-------+-------|

The table below is our belief at the beginning of each day, about
which station is overall best in predicting rain. What should our
initial belief be?

#+Name: Belief at start of day
| Belief        | Day 1 | Day 2 | Day 3 | Day 4 |
|---------------+-------+-------+-------+-------|
| MeteoSuisse   |   90% |       |       |       |
| Chris's Model |   10% |       |       |       |
|---------------+-------+-------+-------+-------|

Write a program that updates the beliefs sequentially given
observations and station predictions.


** Hypothesis testing

*** Homework assignment: Define a data collection and analysis problem
* Data analysis in practice
** The garden of many paths
** Visualising fMRI data
** Visualising GWAS data
*** Homework assignment: Visualisation of a project
* Social issues (if there is time)
** Privacy: randomised response
** Fairness: Experimental analysis
** Safety: Robustness




 7. Now collect according to your protocol.






			  
>>>>>>> 91fb83d62803b6946075fec622e3fa6e482aadd1

